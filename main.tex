\documentclass[sigconf]{acmart} %Recommended doc class from Lins;

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage[ruled,vlined]{algorithm2e}
% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size


% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\title{Robo-maze Blast Report}
\author{DogÃ  Sara}
\author{Widmann Lukas}
\author{Askar Sami}

\begin{abstract}
Bomberman + Evolutionary algorithmens + Tournament Arc goes brr 
\end{abstract}

\begin{document}
\maketitle



\section{Introduction}

\subsection{The game}
\href{https://codeberg.org/chrlns/robo-maze-blast.git}{Robo Maze Blast}, created in 2008 by Kai Ritterbusch and Christian Lins, is a clone of the Bomberman game. Also known as Dynablaster, it is a strategy maze-based video game franchise originally developed by Hudson Soft in 1985.

The general goal of Bomberman is to complete the levels by strategically placing bombs in order to kill enemies and destroy blocks. Some blocks in the path can be destroyed by placing bombs near it, and as the bombs detonate, they will create a burst of vertical and horizontal lines of flames. Except for indestructible blocks, contact with the blast will destroy anything on the screen.

\subsection{Our Goal}
The aim of our project is to explore the efficiency of different Genetic Algorithms to develop 3 agents with strategic competence in the Robo Maze Blast scope, and to compare them by making the agents fight against each other and observe which agent outlives the others more frequently.

\section{Background}
\subsection{Genetic Algorithms}
Genetic Algorithms (GA) are optimization algorithms inspired by the process of natural selection and biological evolution. They are widely used to solve complex optimization and search problems in various domains. 
One of those domains is the gaming side as in this paper. 
\begin{figure}
      \centering
      \includegraphics[width = 1.25\linewidth]{pictures/possibleActionsPlayer.png}
      \caption{\label{fig:possibleActionsPlayer}A diagram on the possible actions of a player}
      \end{figure}
Due to constrained optimization (e.g., state/action of the game), Genetic Algorithms are a perfect choice for this task \cite{popescu2025}: 
\textit{``Genetic Algorithms (GAs) were selected for their ability to handle complex combinatorial optimization problems [...] and to encode domain-specific constraints"} (Section 1).

The core steps of a typical genetic algorithm can be described as follows:
\begin{itemize}
    \item \textbf{Population Base}: Initialize a population from valid chromosomes, i.e. a set of strings that encodes any possible solution. Usually, the initial population is chosen randomly.
    \item \textbf{Evaluation}: Each population solution is evaluated on the basis of a predetermined fitness function.
    \item \textbf{Selection}: Reproductive opportunities are allocated to the chromosomes that represent a better solution to the target problem, and such solutions are selected to form a 'mating pool' for the next generation.
    \item \textbf{Crossover and Mutation}: The selected individuals are then combined to produce offspring by exchanging genetic material. Sometimes small changes can happen in the genetic material, such as bit flips. All of this ensures good exploration of the solution space and diversity.
    
\end{itemize}
These steps are repeated for a number of times until an ending criterion is reached.

\begin{figure}
\centering
\includegraphics[width = 0.4\linewidth]{pictures/Steps-of-Genetic-Algorithms.png}
\caption{\label{fig:Steps-of-Genetic-Algorithm}A diagram on the steps of a genetic algorithm}
\end{figure}


\subsection{Robo Maze Blast's Default AI Agent}
The game has its own AI agents that will play against the player in the absence of in-real-life adversaries. They share a common behavior and reasoning that can be visualized with the finite-state machine, as shown in 
Figure 3.

%\begin{figure}
%\includegraphics[width = 1\linewidth]{pictures/bomberman_finite_state_machine 1.png}
%\caption{\label{fig:bomberman_finite_state_machine 1}Finite-state machine of game's AI agent}
%\end{figure}



\section{Fine-tuning agent behaviors with jenetics}


\subsection{Differential Evolution}

\subsection{Agent Behavior}

\subsection{The Reward Metrics}
The fitness function determines which agent behaviors are rewarded, and which are penalized.
\begin{table}[htbp]
\centering

\caption{Agent Reward and Penalty Values}

\begin{tabular}{l|l}
\textbf{Action} & \textbf{Points}  \\
\hline
Movement & +1 (per step) \\
\hline
Place Bomb & +75 (per bomb) \\
\hline
Blow Wall & +150 (per wall) \\
\hline
Kills & +750 (per player) \\
\hline
Death  & -500  \\
\hline
Suicide & -500 \\
\hline
Win without kills & +200  \\
\hline
Win with kills & +1000  \\
\hline
\end{tabular}
\end{table}


\section{Supervised Learning with Jenetics}
\subsection{Introduction}
My objective was to create an AI player using a genetic algorithm (GA) trained on human gameplay data, with an exploration of skill transferability across gaming domains. 
This approach was motivated by competitive gaming frameworks where player strength is quantified through Elo rating systems \cite{elo1978}. 
As an experienced fighting game player, my current Tekken 8 Elo rating stands at \textbf{1940} (profile: \url{https://wank.wavu.wiki/player/3nyHJQr8Gq6Q}, accessed \today). 
While acknowledging that direct skill transfer between a 3D fighting game (Tekken) and a 2D grid-based strategy game (Robo Maze Blast) may be limited, this project tests the hypothesis that:
\begin{center}
    \fbox{\parbox{0.9\linewidth}{
    \textbf{Hypothesis}: Strategic decision-making patterns in high-Elo players exhibit domain-agnostic qualities, such that:
    \begin{equation*}
    \text{Elo}{\text{source}} \geq 1900 \xrightarrow{\text{transfer}} \text{AI}{\text{target}} > \text{AI}_{\text{baseline}} + \Delta
    \end{equation*}
    where $\Delta$ represents measurable skill advantage in Robo Maze Blast.
    }}
\end{center}
%\[
%\text{Mutation}(x) = x + \mathcal{N}(0,\, \sigma^2) \quad \text{where } \sigma = 0.2
%\]
\subsection{Jenetics}
Jenetics is an open-source Java library that provides a genetic Algorithm (GA) framework for solving optimization problems. It abstracts biological evolution principles, such as selection, crossover, and mutation, into reusable software components, enabling users to evolve solutions without implementing a GA from scratch. 
\subsection{Jenetics Framework}
\label{subsec:jenetics}

We selected it for supervised learning because:
\begin{itemize}
    \item \textbf{Prior experience}: We'd successfully used it in previous labs
    \item \textbf{Java compatibility}: Integrated smoothly with our game codebase
    \item \textbf{Rapid adjustments}: Changing parameters takes minutes instead of hours
    \item \textbf{Easy versioning}: Git checkpoints for different configurations
\end{itemize}

\textbf{Key Features We Utilized:}
\begin{itemize}
    \item \textbf{Evolutionary engine}: Automatically handles generations and survival mechanics
    \item \textbf{Domain flexibility}: Worked directly with our game strategy optimization
    \item \textbf{Pre-built operators}: 
    \begin{itemize}
        \item Tournament selection (picks winners from random groups)
        \item Gaussian mutation (makes small, smart adjustments)
        \item Single-point crossover (combines parent solutions)
    \end{itemize}
\end{itemize}

\textbf{Agile Implementation:}
The library let us quickly test configurations by reducing parameters:
\begin{itemize}
    \item Population size: 100 $\rightarrow$ 500 
    \item Mutation rate: 0.1 $\rightarrow$ 0.05 
    \item Training generations: 50 $\rightarrow$ 200 
\end{itemize}

This enables an iterative improvement cycles:
\begin{itemize}

    \item [0.] Gather human data
    \item [1.] Train initial AI
    \item [2.] Identify weaknesses
    \item [3.] Adjust parameters
    \item [4.] Retrain (under 30 minutes)
    

\end{itemize}
\noindent\textbf{Reference}: \href{https://jenetics.io/manual/manual-8.2.0.pdf}{Jenetics User Manual} \cite{jenetics2024}.
\subsection{Implementing Gameplay Recording: Code Changes for Data Collection}
\label{subsec:recording}

To train the AI using Jenetics, we first needed good quality gameplay data. The simplest way to get this data was to record what a human player does during a game - both their actions and the game situation at that moment. This required changes to the Player class, which led to creating the new \texttt{RecordablePlayer} class. The main changes are shown in Algorithm~\ref{alg:recording} and include:

\begin{itemize}
	\item \textbf{Recording trigger}: Making the game log actions when players move or place bombs
	\item \textbf{State capture}: Saving player position and bomb status in simple numbers
	\item \textbf{Data export}: Saving all records to a CSV file for AI training
\end{itemize}

\textbf{Example from actual gameplay:}
Imagine this situation during a game:
\begin{itemize}
	\item Player is at position (7, 12) in a 20x20 grid
	\item Player can place a bomb (has bombs available)
	\item There's a bomb nearby above the player
\end{itemize}

The \texttt{RecordablePlayer} would save this information as:\\
\texttt{0.35, 0.60, 1.0, 1.0, -0.4, 5}

\begin{center}
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Value} & \textbf{Meaning} \\ 
		\hline
		0.35 & Player is 35\% across the level (X position) \\ 
		0.60 & Player is 60\% down the level (Y position) \\ 
		1.0 & Bomb available \\ 
		1.0 & Bomb nearby \\ 
		-0.4 & Bomb is above player \\ 
		5 & Player placed a bomb \\ 
		\hline
	\end{tabular}
\end{center}

This simple number format allows the AI to learn from many such records. In the future I would collect a sample trainings data set to train 

\begin{algorithm}[t]
	\caption{RecordablePlayer Modifications}
	\label{alg:recording}
	\DontPrintSemicolon
	\SetKwFunction{GetState}{getStateVector}
	\SetKwFunction{Save}{saveRecordings}
	
	\textbf{Class} RecordablePlayer \textbf{extends} Player\;
	\nl\textbf{New Data:} 
	recordings = [] \;
	isRecording = false \tcp*{Recording on/off switch}
	
	\BlankLine
	\nl\textbf{Key Changes:}\;
	\nl \textbf{move(dx, dy)}:
	super.move(dx, dy)\;
	\If{isRecording}{\textsc{LogMovement}(dx, dy) \tcp*{Records moves}}
	
	\nl \textbf{placeBomb()}:
	super.placeBomb()\;
	\If{isRecording}{\textsc{LogAction}(BOMB) \tcp*{Records bombs}}
	
	\BlankLine
	\nl\textbf{New Methods:}\;
	\nl\textsc{LogMovement}(dx, dy):
	Convert dx/dy to direction\;
	\textsc{LogAction}(direction)\;
	
	\nl\textsc{LogAction}(action):
	state $\leftarrow$ [normX, normY, bombAvail, bombNear]\;
	recordings $\leftarrow$ recordings + (state, action)\;
	
	\nl\textsc{Save}(filename):
	Write all recordings to file\;
\end{algorithm}
\subsection{How to add Citations and a References List}

You can simply upload a \verb|.bib| file containing your BibTeX entries, created with a tool such as JabRef. You can then cite entries from it, like this: \cite{greenwade93}. Just remember to specify a bibliography style, as well as the filename of the \verb|.bib|. You can find a \href{https://www.overleaf.com/help/97-how-to-include-a-bibliography-using-bibtex}{video tutorial here} to learn more about BibTeX.

If you have an \href{https://www.overleaf.com/user/subscription/plans}{upgraded account}, you can also import your Mendeley or Zotero library directly as a \verb|.bib| file, via the upload menu in the file-tree.

\subsection{Good luck!}


\section{Tree-Based Genetic Programming}
\subsection{Introduction}

The evolutionary algorithm chosen for the second implementation is tree-based genetic programming (GP). This approach offers multiple benefits: 
First, it is highly flexible. The tree structure allows a wide range of programs to be represented by the algorithm. 
Furthermore, GP can "discover novel and unexpected solutions that may not be apparent to human programmers," as the algorithm is not biased toward favoring certain approaches over others, as long as they are within the rules set by the programmer or hardware. 
Additionally, GP can easily adapt to changing requirements and inputs, making it highly adaptable. 
This approach was chosen due to its high flexibility. While the initial training phase against the already implemented AI players might be more challenging due to the complexity of the setup compared to other algorithms like Differential Evolution or regular Genetic Algorithms, the benefits of flexibility and adaptability to new circumstances are promising in an environment where multiple evolutionary algorithms compete against each other. 

\subsection{Design Decisions}

The implementation of the algorithm is encapsulated in the GeneticPlayer class. This class does not inherit from the AIPlayer class, which already contains significant functionality. Inheritance was avoided because the setup of genetic programming differs significantly from "regular" genetic algorithms. GP uses a tree structure and consists of multiple components (functions, terminals, and constants). Terminals represent the leaf nodes of the tree, whereas functions branch out, increasing the complexity of the tree.The GeneticPlayer class contains an abstract static class called Operation, which serves as the base for all terminals and functions. The respective terminals and functions inherit from Operation. The terminals include basic movement operators such as MoveUp, MoveDown, MoveLeft, MoveRight, and Stay, which represent movements in the game along the x- and y-axes. Additional terminals include IsTargetZone, CheckForBomb, PlaceBomb, CheckForExtra, GetDistanceToEnemy, and three constant values. These terminals encapsulate functionality already implemented for the AIPlayer, allowing the GP to iterate over it. The constant values are arbitrary defaults that can be adjusted based on the algorithm's performance or goals.The algorithm also relies on two operations: GreaterThan and IfElse. GreaterThan has an arity of 2, meaning it compares two values, returning 1 if the first value is greater than the second and 0 otherwise. IfElse has an arity of 3. It checks whether the first value is greater than 0. If true, it forwards the value t[1]; otherwise, it forwards t[2].The core of the GP is the initializeGP method. In this method, functions and terminals are initialized, and the maximum tree depth is set. During the training phase, a depth of 5 was used, but this can be adjusted based on performance. The engine combines the algorithm's constraints with the fitness function. Parameters such as population size, mutation rate, and crossover rate can be manually adjusted depending on the algorithm's performance. The result performs the actual training by streaming the data and applying the constraint of the number of generations. Finally, the best strategy is returned.The fitness function is where the actual training occurs. Each time the fitness function is called, a new game and playground are initialized. A new GeneticPlayer and three new AIPlayer instances are created. Notably, during the training phase, a specific method, addAIForFitness, was implemented in the Game class. This method does not start the threads typically used during a game simulation. The operations and terminals are linked to the newly created GeneticPlayer. 

\begin{verbatim}
for (int i = 0; i < 1000; i++) {
            if (!game.isRunning() || goodboy.isDead()) {
                System.err.println("Fitness: Stopped at iteration " + i + ", game running = " + game.isRunning() + ", GoodBoy dead = " + goodboy.isDead());
                steps = i;
                break;
            }
            for (Player player : game.getPlayers()) {
                if (player instanceof AIPlayer aip) {
                    System.err.println("Fitness: AIPlayer " + aip.getNickname() + " tick");
                    aip.tick();
                } else {
                    GeneticPlayer gp = (GeneticPlayer) player;
                    double result = gp.program.eval();
                    System.err.println("Fitness: GeneticPlayer tick, result = " + result + ", position = (" + gp.gridX + "," + gp.gridY + ")");
                    if (result == 1.0 || result == 2.0 || result == 3.0 || result == 4.0) {
                        successfulMoves++;
                    }
                }
            }
            game.tick();
        }
\end{verbatim}

In the loop, the player performs a maximum of 1,000 iterations before being forced to terminate. The loop is interrupted if the player dies or the game is no longer running, simulating a real environment where the GP interacts with AIPlayer instances. In each iteration, both the AIPlayer and GeneticPlayer call their tickmethods, selecting a new action and synchronizing their current state. After all players have synchronized, the game synchronizes with game.tick. During game.tick, selected bombs detonate, extras collected by players in the previous period modify their bomb range or capacity, and players may be killed by exploding bombs and removed from the game. Successful moves by the GP are recorded during the loop, as they impact the overall fitness.The fitness function consists of multiple components: 
\begin{itemize}
      \item survivalTime 
      \item extrasCollected 
      \item deathPenalty 
      \item moveReward 
\end{itemize}
Since the algorithm aims to minimize the fitness function, beneficial actions for the GP receive a negative multiplier, while harmful actions receive a positive multiplier. Notably, the fitness function currently ignores the number of opponents the GP kills, meaning aggressive behavior does not affect fitness. This design choice promotes defensive behavior, prioritizing survival over aggression. The goal is for the player to survive as long as possible, leading to longer games compared to aggressive strategies where the player either dies or kills quickly. This defensive approach, inspired by strategic games like chess where top players prioritize avoiding mistakes over attacking, is reinforced by a high deathPenalty and a strong emphasis on survivalTime as a negative metric.The GeneticPlayer class also includes a tick method, called by the GeneticPlayerThread, which is started in a real game environment to visually observe the player's behavior. 

\subsection{Issues}
The algorithm in its current form does not perform as intended. Several issues arose during implementation, some of which remain unresolved: 
\begin{enumerate}
      \item Not inheriting from the AIPlayer class. 
      The primary issue, likely causing subsequent problems, was the decision not to inherit from the AIPlayerclass. Although this was a deliberate choice initially, it likely caused more harm than good. Much of the functionality was copied with slight adjustments, adding redundant code without significant value. The GeneticPlayer inherits from the Player class, sharing many variables with AIPlayer. This required moving some code, such as the die and isDead methods, from AIPlayer to Player to make it accessible to the GP.
      \item Adjusting classes that were already functional. 
      Other classes, such as the Game class, were modified to include the tick method, which synchronizes player actions within a time frame. However, these adjustments introduced additional issues, complicating debugging.cDuring the project, console output showed that the player performed actions like moving up and down, which sometimes succeeded and sometimes failed depending on obstacles like walls. However, the player never placed bombs. Debug print statements revealed that the condition player.bombs.size() < player.bombCount was never true, indicating that bombs always contained a bomb before the output statement was triggered. This led to replacing many System.out.println() statements with System.err.println(), suspecting that threading issues were affecting the print statements (as threads were initially started for all AIPlayer instances). This hypothesis was confirmed, revealing another issue: the players and bombs were not synchronized.
      \item Synchronization of player behavior. 
      While AIPlayer instances were bound to a tick every 300 ms, bombs exploded every 4,000 ms. The GP, however, was not constrained by these timings, operating only limited by CPU capacity. This caused the GP to perform significantly more iterations before a bomb exploded, leading to poor assumptions about its behavior after placing a bomb, often resulting in the player committing suicide. Ultimately, AIPlayer threads were disabled to remove artificial constraints designed for a realistic gaming experience, allowing the simulation to run without being bound by these limitations. The above issues have left the algorithm only partially functional. Synchronization of triggered behaviors (bomb explosions, AIPlayer actions, and GeneticPlayer actions) caused significant problems. Many issues could have likely been avoided by inheriting most behavior from AIPlayer. The synchronization problem could have been mitigated by aligning the GP's tick time with that of the AIPlayer. However, this would have slowed the algorithm or reduced its quality, as the complexity and population size would need to be significantly reduced to maintain performance. 
\end{enumerate}

We hope you find Overleaf useful, and do take a look at our \href{https://www.overleaf.com/learn}{help library} for more tutorials and user guides! Please also let us know if you have any feedback using the \textbf{Contact us} link at the bottom of the Overleaf menu --- or use the contact form at \url{https://www.overleaf.com/contact}.

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}
